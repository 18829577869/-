"""
å¤šæ•°æ®æºç»„åˆç®¡ç†æ¨¡å—
æ”¯æŒ Tushareã€AkShareã€baostock ç­‰å¤šä¸ªæ•°æ®æºçš„ç»„åˆä½¿ç”¨
å®ç°æ•°æ®æºä¼˜å…ˆçº§ã€å®¹é”™åˆ‡æ¢ã€æ•°æ®åˆå¹¶ç­‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import datetime
import time
from typing import Dict, List, Optional, Tuple
import warnings
import os

# å¯¼å…¥åçˆ¬è™«å·¥å…·
try:
    from .anti_crawler_pool import get_global_pool, setup_akshare_environment, monkey_patch_requests
    ANTI_CRAWLER_AVAILABLE = True
except ImportError:
    try:
        from anti_crawler_pool import get_global_pool, setup_akshare_environment, monkey_patch_requests
        ANTI_CRAWLER_AVAILABLE = True
    except ImportError:
        ANTI_CRAWLER_AVAILABLE = False
        warnings.warn("åçˆ¬è™«å·¥å…·æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯·æ±‚æ–¹å¼")


class MultiDataSourceManager:
    """å¤šæ•°æ®æºç»„åˆç®¡ç†å™¨"""
    
    def __init__(self, 
                 stock_code: str,
                 sources: Optional[List[str]] = None,
                 priority: Optional[List[str]] = None,
                 timeout: int = 10,
                 retry_times: int = 3,
                 enable_anti_crawler: bool = True,
                 proxies: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–å¤šæ•°æ®æºç®¡ç†å™¨
        
        å‚æ•°:
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 'sh.600036' æˆ– '600036'ï¼‰
            sources: å¯ç”¨æ•°æ®æºåˆ—è¡¨ï¼ŒNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            priority: æ•°æ®æºä¼˜å…ˆçº§åˆ—è¡¨ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤ä¼˜å…ˆçº§
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            retry_times: é‡è¯•æ¬¡æ•°
            enable_anti_crawler: æ˜¯å¦å¯ç”¨åçˆ¬è™«åŠŸèƒ½ï¼ˆCookie/UA/ä»£ç†æ± ï¼‰
            proxies: ä»£ç†åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ ['http://user:pass@host:port', ...]
        """
        self.stock_code = stock_code
        self.timeout = timeout
        self.retry_times = retry_times
        self.enable_anti_crawler = enable_anti_crawler and ANTI_CRAWLER_AVAILABLE
        
        # åˆå§‹åŒ–åçˆ¬è™«æ± 
        if self.enable_anti_crawler:
            self.anti_crawler_pool = get_global_pool()
            if proxies:
                self.anti_crawler_pool.add_proxies(proxies)
            # å¯¹requestsè¿›è¡Œmonkey patch
            monkey_patch_requests(self.anti_crawler_pool)
            print(f"ğŸ›¡ï¸  åçˆ¬è™«åŠŸèƒ½å·²å¯ç”¨ (UAæ± : {len(self.anti_crawler_pool.user_agents)}ä¸ª, ä»£ç†æ± : {len(self.anti_crawler_pool.proxies_pool)}ä¸ª)")
        else:
            self.anti_crawler_pool = None
        
        # æ£€æµ‹å¯ç”¨æ•°æ®æº
        self.available_sources = self._detect_available_sources()
        
        # è®¾ç½®æ•°æ®æºåˆ—è¡¨
        if sources is None:
            self.sources = self.available_sources
        else:
            self.sources = [s for s in sources if s in self.available_sources]
        
        # è®¾ç½®ä¼˜å…ˆçº§ï¼ˆé»˜è®¤ï¼štushare > akshare > baostockï¼‰
        if priority is None:
            self.priority = ['tushare', 'akshare', 'baostock']
        else:
            self.priority = [p for p in priority if p in self.available_sources]
        
        # æ•°æ®æºçŠ¶æ€ç»Ÿè®¡
        self.source_stats = {source: {'success': 0, 'fail': 0, 'last_success': None} 
                            for source in self.sources}
        
        # å½“å‰ä½¿ç”¨çš„æ•°æ®æº
        self.current_source = None
        
        print(f"âœ… å¤šæ•°æ®æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å¯ç”¨æ•°æ®æº: {', '.join(self.available_sources)}")
        print(f"   ä¼˜å…ˆçº§é¡ºåº: {' > '.join(self.priority)}")
    
    def _detect_available_sources(self) -> List[str]:
        """æ£€æµ‹å¯ç”¨çš„æ•°æ®æº"""
        available = []
        
        # æ£€æµ‹ Tushare
        try:
            import tushare as ts
            # å°è¯•è·å–tokenï¼ˆå¦‚æœè®¾ç½®äº†ï¼‰
            token = self._get_tushare_token()
            if token:
                ts.set_token(token)
                available.append('tushare')
        except ImportError:
            pass
        
        # æ£€æµ‹ AkShare
        try:
            import akshare as ak
            available.append('akshare')
        except ImportError:
            pass
        
        # æ£€æµ‹ baostock
        try:
            import baostock as bs
            available.append('baostock')
        except ImportError:
            pass
        
        return available
    
    def _get_tushare_token(self) -> Optional[str]:
        """è·å– Tushare tokenï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ï¼‰"""
        import os
        return os.getenv('TUSHARE_TOKEN', '')
    
    def _convert_stock_code(self, code: str, target_source: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼ä»¥é€‚åº”ä¸åŒæ•°æ®æº
        
        å‚æ•°:
            code: åŸå§‹è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 'sh.600036' æˆ– '600036'ï¼‰
            target_source: ç›®æ ‡æ•°æ®æºåç§°
        
        è¿”å›:
            è½¬æ¢åçš„è‚¡ç¥¨ä»£ç 
        """
        # è§£æä»£ç 
        if '.' in code:
            market, num = code.split('.')
        else:
            # æ ¹æ®ä»£ç åˆ¤æ–­å¸‚åœº
            if code.startswith('6'):
                market = 'sh'
                num = code
            else:
                market = 'sz'
                num = code
        
        # è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼
        if target_source == 'tushare':
            return f"{num}.{market.upper()}"
        elif target_source == 'akshare':
            return num
        elif target_source == 'baostock':
            return f"{market}.{num}"
        else:
            return code
    
    def _fetch_from_tushare(self, days: int = 7) -> Optional[pd.DataFrame]:
        """ä» Tushare è·å–æ•°æ®"""
        try:
            import tushare as ts
            token = self._get_tushare_token()
            if not token:
                return None
            
            ts.set_token(token)
            pro = ts.pro_api()
            
            code = self._convert_stock_code(self.stock_code, 'tushare')
            today = datetime.date.today()
            start_date = (today - datetime.timedelta(days=days)).strftime('%Y%m%d')
            end_date = today.strftime('%Y%m%d')
            
            # å°è¯•è·å–5åˆ†é’ŸKçº¿
            try:
                df = pro.stk_mins(
                    ts_code=code,
                    freq='5min',
                    start_date=start_date + '0930',
                    end_date=end_date + '1500'
                )
                if df is not None and len(df) > 0:
                    # è½¬æ¢æ ¼å¼
                    if 'trade_time' in df.columns:
                        df['time'] = pd.to_datetime(df['trade_time']).dt.strftime('%Y%m%d%H%M%S')
                        df['date'] = pd.to_datetime(df['trade_time']).dt.strftime('%Y-%m-%d')
                    df = df.rename(columns={'close': 'close', 'vol': 'volume'})
                    return df[['date', 'time', 'close', 'volume']]
            except:
                # å¦‚æœ5åˆ†é’Ÿæ•°æ®å¤±è´¥ï¼Œå°è¯•æ—¥çº¿æ•°æ®
                df = pro.daily(
                    ts_code=code,
                    start_date=start_date,
                    end_date=end_date
                )
                if df is not None and len(df) > 0:
                    df = df.rename(columns={'trade_date': 'date', 'close': 'close', 'vol': 'volume'})
                    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')
                    df['time'] = df['date'] + '15000000'
                    return df[['date', 'time', 'close', 'volume']]
            
            return None
        except Exception as e:
            warnings.warn(f"Tushare è·å–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _fetch_from_akshare(self, days: int = 7) -> Optional[pd.DataFrame]:
        """ä» AkShare è·å–æ•°æ®ï¼ˆå¸¦åçˆ¬è™«ä¿æŠ¤ï¼‰"""
        # è®¾ç½®åçˆ¬è™«ç¯å¢ƒ
        if self.enable_anti_crawler and self.anti_crawler_pool:
            setup_akshare_environment(self.anti_crawler_pool)
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            self.anti_crawler_pool.random_delay(0.5, 1.5)
        
        max_retries = self.retry_times
        for attempt in range(max_retries):
            try:
                import akshare as ak
                
                code = self._convert_stock_code(self.stock_code, 'akshare')
                today = datetime.date.today()
                start_date = (today - datetime.timedelta(days=days)).strftime('%Y%m%d')
                end_date = today.strftime('%Y%m%d')
                
                # è·å–5åˆ†é’ŸKçº¿
                try:
                    df = ak.stock_zh_a_hist_min_em(
                        symbol=code,
                        period="5",
                        adjust="qfq",
                        start_date=start_date,
                        end_date=end_date
                    )
                    if df is not None and len(df) > 0:
                        # è½¬æ¢åˆ—å
                        column_mapping = {
                            'æ—¶é—´': 'time',
                            'æ”¶ç›˜': 'close',
                            'æˆäº¤é‡': 'volume',
                            'æ—¥æœŸ': 'date'
                        }
                        for old_col, new_col in column_mapping.items():
                            if old_col in df.columns:
                                df = df.rename(columns={old_col: new_col})
                        
                        if 'time' in df.columns:
                            df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y%m%d%H%M%S')
                            df['date'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d')
                        elif 'date' in df.columns:
                            df['time'] = pd.to_datetime(df['date']).dt.strftime('%Y%m%d%H%M%S')
                            df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
                        
                        return df[['date', 'time', 'close', 'volume']]
                except Exception as e1:
                    # å¦‚æœ5åˆ†é’Ÿæ•°æ®å¤±è´¥ï¼Œå°è¯•æ—¥çº¿æ•°æ®
                    if attempt < max_retries - 1:
                        # é‡è¯•å‰åˆ‡æ¢ä»£ç†/UA
                        if self.enable_anti_crawler and self.anti_crawler_pool:
                            setup_akshare_environment(self.anti_crawler_pool)
                            self.anti_crawler_pool.random_delay(1.0, 2.0)
                        continue
                    
                    try:
                        df = ak.stock_zh_a_hist(
                            symbol=code,
                            period="daily",
                            start_date=start_date,
                            end_date=end_date,
                            adjust="qfq"
                        )
                        if df is not None and len(df) > 0:
                            df = df.rename(columns={'æ—¥æœŸ': 'date', 'æ”¶ç›˜': 'close', 'æˆäº¤é‡': 'volume'})
                            df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
                            df['time'] = df['date'] + '15000000'
                            return df[['date', 'time', 'close', 'volume']]
                    except Exception as e2:
                        if attempt < max_retries - 1:
                            # é‡è¯•å‰åˆ‡æ¢ä»£ç†/UA
                            if self.enable_anti_crawler and self.anti_crawler_pool:
                                setup_akshare_environment(self.anti_crawler_pool)
                                self.anti_crawler_pool.random_delay(1.0, 2.0)
                            continue
                        raise e2
                
                return None
            except (ConnectionError, TimeoutError, OSError) as e:
                # ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œå°è¯•é‡è¯•
                if attempt < max_retries - 1:
                    # åˆ‡æ¢ä»£ç†/UAåé‡è¯•
                    if self.enable_anti_crawler and self.anti_crawler_pool:
                        setup_akshare_environment(self.anti_crawler_pool)
                        self.anti_crawler_pool.random_delay(2.0, 4.0)
                    continue
                # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œé™é»˜å¤„ç†
                return None
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼Œä¸é‡è¯•
                if attempt < max_retries - 1:
                    if self.enable_anti_crawler and self.anti_crawler_pool:
                        setup_akshare_environment(self.anti_crawler_pool)
                        self.anti_crawler_pool.random_delay(1.0, 2.0)
                    continue
                # é™é»˜å¤„ç†ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œé¿å…è¿‡å¤šè­¦å‘Š
                return None
        
        return None
    
    def _fetch_from_baostock(self, days: int = 7) -> Optional[pd.DataFrame]:
        """ä» baostock è·å–æ•°æ®"""
        try:
            import baostock as bs
            
            code = self._convert_stock_code(self.stock_code, 'baostock')
            today = datetime.date.today()
            start_date = (today - datetime.timedelta(days=days)).strftime('%Y-%m-%d')
            end_date = today.strftime('%Y-%m-%d')
            
            bs.login()
            try:
                rs = bs.query_history_k_data_plus(
                    code,
                    "date,time,close,volume",
                    start_date=start_date,
                    end_date=end_date,
                    frequency='5',
                    adjustflag='3'
                )
                
                if rs.error_code != '0':
                    return None
                
                data_list = []
                while rs.next():
                    data_list.append(rs.get_row_data())
                
                if not data_list:
                    return None
                
                df = pd.DataFrame(data_list, columns=rs.fields)
                
                # æ·»åŠ æ—¥æœŸåˆ—
                # æ·»åŠ æ—¥æœŸåˆ—
                if 'time' in df.columns:
                    try:
                        # å°è¯•è§£ææ—¶é—´æ ¼å¼
                        if df['time'].dtype == 'object':
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œå°è¯•å¤šç§æ ¼å¼
                            # å…ˆå°è¯•æŒ‡å®šæ ¼å¼ï¼Œé¿å…è­¦å‘Š
                            try:
                                time_parsed = pd.to_datetime(df['time'], format='%Y%m%d%H%M%S', errors='coerce')
                                if time_parsed.isna().all():
                                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–å¸¸è§æ ¼å¼
                                    for fmt in ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%Y-%m-%d']:
                                        time_parsed = pd.to_datetime(df['time'], format=fmt, errors='coerce')
                                        if not time_parsed.isna().all():
                                            break
                                    # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨dateutilï¼ˆä¼šäº§ç”Ÿè­¦å‘Šï¼Œä½†è‡³å°‘èƒ½è§£æï¼‰
                                    if time_parsed.isna().all():
                                        with warnings.catch_warnings():
                                            warnings.simplefilter('ignore', UserWarning)
                                            time_parsed = pd.to_datetime(df['time'], errors='coerce')
                            except Exception:
                                # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨dateutilï¼ˆä¼šäº§ç”Ÿè­¦å‘Šï¼‰
                                with warnings.catch_warnings():
                                    warnings.simplefilter('ignore', UserWarning)
                                    time_parsed = pd.to_datetime(df['time'], errors='coerce')
                            df['date'] = time_parsed.dt.strftime('%Y-%m-%d')
                        else:
                            df['date'] = pd.to_datetime(df['time'], errors='coerce').dt.strftime('%Y-%m-%d')
                    except:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                        df['date'] = datetime.date.today().strftime('%Y-%m-%d')
                
                return df
            finally:
                bs.logout()
                
        except Exception as e:
            warnings.warn(f"baostock è·å–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def fetch_data(self, days: int = 7, use_cache: bool = True) -> Tuple[Optional[pd.DataFrame], str]:
        """
        ä»å¤šä¸ªæ•°æ®æºè·å–æ•°æ®ï¼ˆæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼‰
        
        å‚æ•°:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆæœªæ¥å®ç°ï¼‰
        
        è¿”å›:
            (æ•°æ®DataFrame, æ•°æ®æºåç§°) çš„å…ƒç»„
        """
        # æŒ‰ä¼˜å…ˆçº§å°è¯•å„ä¸ªæ•°æ®æº
        for source in self.priority:
            if source not in self.sources:
                continue
            
            try:
                df = None
                if source == 'tushare':
                    df = self._fetch_from_tushare(days)
                elif source == 'akshare':
                    df = self._fetch_from_akshare(days)
                elif source == 'baostock':
                    df = self._fetch_from_baostock(days)
                
                if df is not None and len(df) > 0:
                    # æ›´æ–°ç»Ÿè®¡
                    self.source_stats[source]['success'] += 1
                    self.source_stats[source]['last_success'] = datetime.datetime.now()
                    self.current_source = source
                    return df, source
                else:
                    self.source_stats[source]['fail'] += 1
            except Exception as e:
                self.source_stats[source]['fail'] += 1
                warnings.warn(f"æ•°æ®æº {source} è·å–å¤±è´¥: {e}")
        
        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        return None, "none"
    
    def merge_data_from_multiple_sources(self, 
                                         days: int = 7,
                                         merge_strategy: str = 'priority') -> Optional[pd.DataFrame]:
        """
        ä»å¤šä¸ªæ•°æ®æºåˆå¹¶æ•°æ®
        
        å‚æ•°:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            merge_strategy: åˆå¹¶ç­–ç•¥ ('priority': æŒ‰ä¼˜å…ˆçº§, 'union': å–å¹¶é›†, 'intersect': å–äº¤é›†)
        
        è¿”å›:
            åˆå¹¶åçš„ DataFrame
        """
        all_data = {}
        
        # ä»æ‰€æœ‰å¯ç”¨æ•°æ®æºè·å–æ•°æ®
        for source in self.sources:
            try:
                df = None
                if source == 'tushare':
                    df = self._fetch_from_tushare(days)
                elif source == 'akshare':
                    df = self._fetch_from_akshare(days)
                elif source == 'baostock':
                    df = self._fetch_from_baostock(days)
                
                if df is not None and len(df) > 0:
                    all_data[source] = df
            except:
                pass
        
        if not all_data:
            return None
        
        # æ ¹æ®ç­–ç•¥åˆå¹¶
        if merge_strategy == 'priority':
            # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°æ®æº
            for source in self.priority:
                if source in all_data:
                    return all_data[source]
        
        elif merge_strategy == 'union':
            # å–å¹¶é›†ï¼ˆåˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œå»é‡ï¼‰
            merged = pd.concat(all_data.values(), ignore_index=True)
            merged = merged.drop_duplicates(subset=['time'], keep='last')
            merged = merged.sort_values('time')
            return merged
        
        elif merge_strategy == 'intersect':
            # å–äº¤é›†ï¼ˆåªä¿ç•™æ‰€æœ‰æ•°æ®æºéƒ½æœ‰çš„æ—¶é—´ç‚¹ï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å›ä¼˜å…ˆçº§æœ€é«˜çš„æ•°æ®æº
            for source in self.priority:
                if source in all_data:
                    return all_data[source]
        
        return None
    
    def get_source_stats(self) -> Dict:
        """è·å–æ•°æ®æºç»Ÿè®¡ä¿¡æ¯"""
        return self.source_stats.copy()
    
    def get_current_source(self) -> Optional[str]:
        """è·å–å½“å‰ä½¿ç”¨çš„æ•°æ®æº"""
        return self.current_source

