# V8 配置指南

## 📋 完整配置流程

### 第一步：获取 API 密钥

#### 方案 A: DeepSeek（推荐）

1. **注册账号**
   - 访问：https://platform.deepseek.com
   - 使用手机号或邮箱注册

2. **创建 API Key**
   - 登录后进入「API 管理」
   - 点击「创建 API Key」
   - 复制并保存密钥（只显示一次）

3. **充值**
   - 进入「账户充值」
   - 最低充值 10 元
   - 支持微信/支付宝

4. **价格参考**
   ```
   DeepSeek-Chat:
   - 输入：0.001 元/千tokens
   - 输出：0.002 元/千tokens
   
   每次调用成本约 0.001 元
   ```

#### 方案 B: Grok

1. **获取访问权限**
   - 访问：https://x.ai
   - 需要 X Premium+ 订阅
   - 或申请 API 访问权限

2. **创建 API Key**
   - 进入开发者平台
   - 创建新的 API Key
   - 保存密钥

3. **价格参考**
   ```
   Grok-Beta:
   - 按请求收费
   - 具体价格见官网
   ```

---

### 第二步：配置环境变量

#### Windows (PowerShell)

**临时设置**（仅当前会话）:

```powershell
$env:DEEPSEEK_API_KEY = "your_deepseek_api_key_here"
```

**永久设置**:

```powershell
# 方法 1: 通过 PowerShell
[System.Environment]::SetEnvironmentVariable('DEEPSEEK_API_KEY', 'your_key_here', 'User')

# 方法 2: 通过系统设置
# 1. 右键「此电脑」→「属性」
# 2. 点击「高级系统设置」
# 3. 点击「环境变量」
# 4. 在「用户变量」中新建:
#    变量名: DEEPSEEK_API_KEY
#    变量值: your_key_here
# 5. 重启 PowerShell
```

#### Linux / macOS

**临时设置**:

```bash
export DEEPSEEK_API_KEY="your_deepseek_api_key_here"
```

**永久设置**:

```bash
# 添加到 ~/.bashrc 或 ~/.zshrc
echo 'export DEEPSEEK_API_KEY="your_key_here"' >> ~/.bashrc
source ~/.bashrc
```

#### 验证配置

```powershell
# Windows
echo $env:DEEPSEEK_API_KEY

# Linux/macOS
echo $DEEPSEEK_API_KEY
```

---

### 第三步：选择训练模式

#### 模式 A: 完全模拟（免费，推荐新手）

**特点**:
- ✅ 无需 API 密钥
- ✅ 完全免费
- ✅ 训练速度快
- ✅ 数据一致性好
- ❌ 无法反映最新市场变化

**配置**:

```python
# train_v8.py 中设置
LLM_API_KEY = None  # 保持为 None
```

**使用场景**:
- 模型开发和调试
- 超参数调优
- 策略验证

---

#### 模式 B: 混合模式（推荐生产）

**特点**:
- ✅ 训练用模拟（便宜）
- ✅ 实盘用真实 LLM（准确）
- ⚖️ 成本可控

**配置**:

```python
# 训练时
LLM_API_KEY = None

# 实盘时
LLM_API_KEY = "your_actual_key"
```

**使用场景**:
- 推荐大多数用户
- 成本敏感用户
- 需要实盘准确性

---

#### 模式 C: 完全真实（最准确，成本高）

**特点**:
- ✅ 训练和实盘都用真实 LLM
- ✅ 最准确的市场感知
- ❌ 成本较高（约 2-5 元/次训练）

**配置**:

```python
# train_v8.py 中设置
LLM_API_KEY = "your_deepseek_api_key"

# 或使用环境变量
LLM_API_KEY = None  # 会自动读取环境变量
```

**使用场景**:
- 高频交易策略
- 对市场敏感度要求高
- 预算充足用户

---

### 第四步：批量生成市场情报

#### 为什么需要批量生成？

训练时会访问历史 2000+ 天的数据，如果每天都调用 LLM：
- **模拟模式**: 每次训练都重新生成 → 浪费时间
- **真实 API**: 每次训练都调用 → 浪费钱

**解决方案**: 预先批量生成并缓存。

#### 操作步骤

**1. 创建批量生成脚本**

`generate_intelligence.py`:

```python
from llm_market_intelligence import MarketIntelligenceAgent

# 初始化代理
agent = MarketIntelligenceAgent(
    provider="deepseek",
    enable_cache=True
)

# 批量生成（使用模拟数据）
agent.batch_generate_intelligence(
    start_date="2020-01-01",
    end_date="2025-11-24",
    use_mock=True  # 模拟模式，完全免费
)

print("完成！缓存已保存到 market_intelligence_cache/")
```

**2. 运行生成**

```bash
python generate_intelligence.py
```

输出示例：

```
======================================================================
批量生成市场情报: 2020-01-01 至 2025-11-24
总计: 2155 天
模式: 模拟数据
======================================================================

[进度] 100/2155 完成
[进度] 200/2155 完成
...
[完成] 成功生成 2155 天的市场情报
[缓存] ./market_intelligence_cache/
```

**3. 验证缓存**

```bash
# Windows PowerShell
ls market_intelligence_cache/ | Measure-Object -line

# Linux/macOS
ls market_intelligence_cache/ | wc -l
```

应该看到 2000+ 个 JSON 文件。

---

### 第五步：配置训练参数

编辑 `train_v8.py`：

#### 基础配置

```python
# === LLM 配置 ===
LLM_PROVIDER = "deepseek"  # 或 "grok"
LLM_API_KEY = None  # None = 模拟模式 或 从环境变量读取
INITIAL_BALANCE = 100000  # 初始资金（元）

# === 训练配置 ===
TOTAL_TIMESTEPS = 3_000_000  # 总训练步数
N_ENVS = 8  # 并行环境数（建议 = CPU核心数 - 2）
SAVE_FREQ = 100_000  # 每多少步保存一次检查点

# === 股票池配置 ===
STOCK_FILES = [
    "stockdata_v7/train/sh.600036.招商银行.csv",
    "stockdata_v7/train/sh.601838.成都银行.csv",
    # ... 添加更多股票
]
```

#### 根据硬件调整

**CPU 配置建议**:

| CPU 核心数 | N_ENVS | TOTAL_TIMESTEPS | 预计时间 |
|-----------|--------|-----------------|---------|
| 4 核 | 2-3 | 1,500,000 | 3-4 小时 |
| 8 核 | 6-8 | 3,000,000 | 2-3 小时 |
| 16 核 | 12-14 | 5,000,000 | 2-3 小时 |

**内存配置建议**:

- 最低：8 GB
- 推荐：16 GB
- 多股票 + 大并发：32 GB

#### PPO 超参数调优

根据您的股票特性调整：

**高波动股票**（科技、小盘）:

```python
PPO_PARAMS = {
    "learning_rate": 2e-4,  # 降低学习率
    "ent_coef": 0.02,  # 增加探索
    "clip_range": 0.15,  # 更保守的更新
    ...
}

# 在 stock_env_v8.py 中
llm_weight = 0.5  # 增加 LLM 权重
```

**低波动股票**（银行、公用事业）:

```python
PPO_PARAMS = {
    "learning_rate": 5e-4,  # 增加学习率
    "ent_coef": 0.005,  # 减少探索
    "clip_range": 0.25,  # 更激进的更新
    ...
}

# 在 stock_env_v8.py 中
llm_weight = 0.2  # 降低 LLM 权重
```

---

### 第六步：开始训练

#### 完整训练流程

```bash
# 1. 激活虚拟环境
cd E:\wangshub\RL-Stock
.\venv\Scripts\Activate.ps1

# 2. 检查数据
ls stockdata_v7/train/

# 3. 生成市场情报缓存（首次）
python generate_intelligence.py

# 4. 开始训练
python train_v8.py
```

#### 监控训练

**终端监控**:

训练过程中会看到：

```
[1/4] 创建训练环境...
  完成，耗时: 15.3秒

[2/4] 创建评估环境...
[3/4] 初始化 PPO 模型...
  模型参数:
    learning_rate: 0.0003
    n_steps: 2048
    ...

[4/4] 开始训练...
======================================================================

---------------------------------
| time/              |          |
|    fps             | 234      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
| train/             |          |
|    entropy_loss    | -1.94    |
|    learning_rate   | 0.0003   |
|    policy_loss     | -0.0034  |
|    value_loss      | 0.0345   |
---------------------------------
```

**TensorBoard 监控**:

打开新终端：

```bash
tensorboard --logdir=./logs_v8/
```

浏览器访问 http://localhost:6006，查看：
- `eval/mean_reward`: 应逐渐上升
- `train/entropy_loss`: 探索程度
- `rollout/ep_rew_mean`: 回合平均奖励

#### 训练中断与恢复

**中断训练**:

按 `Ctrl+C` 优雅退出，会保存当前状态。

**恢复训练**:

```python
# 修改 train_v8.py 的 train() 函数：

# 加载已有模型
model = PPO.load(
    "ppo_stock_v8.zip",  # 或最新的 checkpoint
    env=env,
    tensorboard_log="./logs_v8/"
)

# 继续训练
model.learn(
    total_timesteps=1_000_000,  # 额外训练 100 万步
    reset_num_timesteps=False,  # 不重置计数器
    callback=[checkpoint_callback, eval_callback]
)
```

---

### 第七步：评估模型

#### 自动评估

训练完成后自动评估所有测试集股票：

```
======================================================================
V8 模型评估
======================================================================

✓ sh.600036.招商银行
  最终净值: 112,450 元
  总收益率: +12.45%
  最大回撤: 5.30%
  夏普比率: 1.58
  交易次数: 8
  胜率: 62.50%
  风险事件: 0

...

======================================================================
汇总统计
======================================================================
平均收益率: +13.20%
平均最大回撤: 6.10%
平均夏普比率: 1.52
平均胜率: 58.33%
总交易次数: 45
总风险事件: 3

[保存] 详细结果: results_v8_20251124_153045.csv
```

#### 手动评估

```bash
python train_v8.py eval
```

#### 单股票详细测试

创建 `test_single_v8.py`:

```python
from stable_baselines3 import PPO
from stock_env_v8 import StockTradingEnvV8

# 加载模型
model = PPO.load("ppo_stock_v8.zip")

# 创建环境
env = StockTradingEnvV8(
    data_file="stockdata_v7/test/sh.600036.招商银行.csv",
    llm_provider="deepseek"
)

obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    env.render()  # 打印每一步
    print(f"  动作: {info['action']}, 奖励: {reward:+.3f}")
    print(f"  市场风险: {info['market_risk']:.2f}, 市场情绪: {info['market_sentiment']:+.2f}\n")

print(f"\n最终净值: {env.net_worth:,.0f} 元")
print(f"总收益率: {(env.net_worth - 100000) / 100000 * 100:+.2f}%")
```

运行：

```bash
python test_single_v8.py
```

---

## 🔧 高级配置

### 自定义市场情报维度

如果您想添加自己的分析维度：

**1. 修改 `llm_market_intelligence.py`**

在 `_call_llm_api()` 中修改 prompt：

```python
prompt = f"""分析 {date} 的A股市场，返回以下评分：

1. 宏观经济评分 (macro_economic_score): [-1, 1]
2. 市场情绪评分 (market_sentiment_score): [-1, 1]
3. 风险等级 (risk_level): [0, 1]
4. 政策影响评分 (policy_impact_score): [-1, 1]
5. 突发事件影响 (emergency_impact_score): [-1, 1]
6. 资金流向评分 (capital_flow_score): [-1, 1]
7. 国际联动系数 (international_correlation): [0, 1]
8. VIX水平 (vix_level): [10, 40]

【新增】
9. 行业景气度 (industry_prosperity): [-1, 1]
10. 技术面强度 (technical_strength): [-1, 1]

返回 JSON:
{{
  "macro_economic_score": 0.2,
  ...
  "industry_prosperity": 0.5,
  "technical_strength": 0.3
}}"""
```

**2. 修改 `get_feature_vector()`**

```python
def get_feature_vector(self, intelligence: Dict) -> list:
    return [
        intelligence.get("macro_economic_score", 0.0),
        intelligence.get("market_sentiment_score", 0.0),
        intelligence.get("risk_level", 0.5),
        intelligence.get("policy_impact_score", 0.0),
        intelligence.get("emergency_impact_score", 0.0),
        intelligence.get("capital_flow_score", 0.0),
        intelligence.get("international_correlation", 0.5),
        intelligence.get("vix_level", 20.0) / 40.0,
        # 新增
        intelligence.get("industry_prosperity", 0.0),
        intelligence.get("technical_strength", 0.0),
    ]
```

**3. 修改 `stock_env_v8.py`**

更新观察空间维度：

```python
# 原: 29 维 (21 技术 + 8 LLM)
# 新: 31 维 (21 技术 + 10 LLM)
self.observation_space = gym.spaces.Box(
    low=-np.inf,
    high=np.inf,
    shape=(31,),  # 更新维度
    dtype=np.float32
)
```

### 自定义奖励函数

如果您想调整 LLM 信号的使用方式：

编辑 `stock_env_v8.py` 的 `_calculate_reward_v8()`:

```python
# 例如：更强调宏观经济
if intelligence:
    macro_score = intelligence.get('macro_economic_score', 0)
    
    # 在好的宏观环境下大幅奖励持仓
    if macro_score > 0.5 and position_ratio > 0.7:
        reward += 0.5 * self.llm_weight  # 原来是 0.1
    
    # 在差的宏观环境下严厉惩罚持仓
    elif macro_score < -0.5 and position_ratio > 0.3:
        reward -= 0.8 * self.llm_weight  # 原来是 0.2
```

### 多 LLM 融合

同时使用 DeepSeek 和 Grok，取平均：

```python
# 在 stock_env_v8.py 中
class StockTradingEnvV8(gym.Env):
    def __init__(self, ..., use_ensemble=False):
        if use_ensemble:
            self.llm_agents = [
                MarketIntelligenceAgent(provider="deepseek"),
                MarketIntelligenceAgent(provider="grok"),
            ]
        else:
            self.llm_agents = [MarketIntelligenceAgent(provider=llm_provider)]
    
    def _get_observation(self):
        # ...
        # 多个 LLM 的平均
        intelligences = [
            agent.get_market_intelligence(date_str)
            for agent in self.llm_agents
        ]
        
        # 取平均
        avg_intelligence = {}
        for key in intelligences[0].keys():
            if isinstance(intelligences[0][key], (int, float)):
                avg_intelligence[key] = np.mean([i[key] for i in intelligences])
        
        llm_features = self.llm_agents[0].get_feature_vector(avg_intelligence)
        # ...
```

---

## 📊 成本计算器

### DeepSeek 成本估算

**训练阶段**（使用缓存，只计算首次生成）:

```
历史数据天数: 2000 天
每天调用成本: 0.001 元
总成本: 2000 × 0.001 = 2 元
```

**实盘阶段**:

```
每日调用次数: 1 次（盘前分析）
每次成本: 0.001 元
月成本: 22 × 0.001 = 0.022 元
年成本: 250 × 0.001 = 0.25 元
```

### Grok 成本估算

Grok 按请求收费，具体价格请查看官网。

### 节省成本的方法

1. **训练时使用模拟数据**（免费）
2. **实盘时才用真实 API**（每日 0.001 元）
3. **多模型共享缓存**（避免重复调用）
4. **批量预生成**（一次性成本）

---

## 🚨 故障排查

### 问题 1: API 调用失败

**症状**:

```
[错误] API 调用失败: 401
[回退] 使用模拟数据
```

**原因**:
- API 密钥无效或过期
- 账户余额不足

**解决**:

```bash
# 检查环境变量
echo $env:DEEPSEEK_API_KEY  # Windows
echo $DEEPSEEK_API_KEY  # Linux/macOS

# 检查余额
# 登录 https://platform.deepseek.com 查看账户余额

# 重新设置密钥
$env:DEEPSEEK_API_KEY = "new_key_here"
```

### 问题 2: 缓存未生效

**症状**:

```
训练时每次都显示 [成功] 获取 XXX 天数据
```

**原因**:
- 缓存目录不存在或权限问题

**解决**:

```bash
# 检查缓存目录
ls market_intelligence_cache/

# 手动创建
mkdir market_intelligence_cache

# 检查权限（Linux/macOS）
chmod 755 market_intelligence_cache/
```

### 问题 3: 训练显存/内存不足

**症状**:

```
MemoryError: Unable to allocate XXX GB
```

**解决**:

```python
# 减少并行环境数
N_ENVS = 2  # 原来可能是 8

# 减少股票数量
STOCK_FILES = STOCK_FILES[:3]  # 只用前 3 只

# 减少批次大小
PPO_PARAMS = {
    ...
    "batch_size": 128,  # 原来可能是 256
    "n_steps": 1024,    # 原来可能是 2048
}
```

### 问题 4: TensorBoard 无法打开

**症状**:

```
TensorBoard 页面显示 "No dashboards are active"
```

**解决**:

```bash
# 检查日志目录
ls logs_v8/

# 确保训练已开始并生成日志
# 刷新 TensorBoard 页面

# 或重新启动 TensorBoard
tensorboard --logdir=./logs_v8/ --reload_interval=5
```

### 问题 5: 模型性能很差

**症状**:

```
平均收益率: -5.20%
```

**可能原因及解决**:

1. **训练不足**: 增加 `TOTAL_TIMESTEPS` 到 500 万
2. **过度探索**: 降低 `ent_coef` 到 0.005
3. **LLM 权重不当**: 调整 `llm_weight` (0.2-0.5)
4. **股票不匹配**: 检查股票池是否适合当前策略
5. **测试集泄漏**: 确保训练集和测试集严格分离

---

## 📈 性能调优清单

### 训练前检查

- [ ] 数据完整性（训练集 > 1000 条）
- [ ] 市场情报缓存已生成
- [ ] 环境变量已设置（如使用真实 API）
- [ ] 硬件资源充足（内存 > 8GB）

### 训练中监控

- [ ] TensorBoard `eval/mean_reward` 上升趋势
- [ ] `train/entropy_loss` 稳定在 -1.5 到 -2.5
- [ ] `train/value_loss` 逐渐下降
- [ ] 定期检查检查点（models_v8/）

### 训练后评估

- [ ] 平均收益率 > 5%
- [ ] 平均最大回撤 < 10%
- [ ] 平均夏普比率 > 1.0
- [ ] 所有股票都有交易（避免"不交易"策略）
- [ ] 风险事件控制在合理范围（< 每只 50 次）

---

## 🎓 最佳实践

### 1. 渐进式训练

```
第 1 阶段（1M 步）: 少量股票（2-3只），模拟 LLM
第 2 阶段（2M 步）: 增加股票（5-8只），模拟 LLM
第 3 阶段（1M 步）: 全部股票，真实 LLM 精调
```

### 2. 定期回测

```bash
# 每 50 万步评估一次
python train_v8.py eval

# 对比不同检查点
python compare_v8_checkpoints.py
```

### 3. 多版本对比

```bash
# 保留多个版本
mv ppo_stock_v8.zip ppo_stock_v8_20251124.zip

# 对比性能
python compare_models_v8.py
```

### 4. 日志分析

```python
# 分析训练日志
import pandas as pd

# 读取 TensorBoard 日志
from tensorboard.backend.event_processing import event_accumulator

ea = event_accumulator.EventAccumulator('logs_v8/PPO_1')
ea.Reload()

# 提取 mean_reward
rewards = pd.DataFrame(ea.Scalars('eval/mean_reward'))
print(rewards.tail(10))
```

---

## 📞 获取帮助

如果遇到问题，请检查：

1. ✅ 是否按照本文档的顺序配置
2. ✅ 环境变量是否正确设置
3. ✅ API 密钥是否有效
4. ✅ 数据文件是否存在
5. ✅ Python 版本 >= 3.9

---

**配置完成后，您就可以开始训练 V8 模型了！** 🎉



